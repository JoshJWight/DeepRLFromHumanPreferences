{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad0c8bd2",
   "metadata": {},
   "source": [
    "Below: the stuff from huggingface deep rl lesson 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56cfcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pybullet_envs\n",
    "\n",
    "import os\n",
    "\n",
    "from huggingface_sb3 import load_from_hub, package_to_hub\n",
    "\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "import torch \n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cad2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"AntBulletEnv-v0\"\n",
    "# Create the env\n",
    "env = gym.make(env_id)\n",
    "\n",
    "# Get the state space and action space\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de311945",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_vec_env(env_id, n_envs=4)\n",
    "\n",
    "# Adding this wrapper to normalize the observation and the reward\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=10.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c90676",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C(policy = \"MlpPolicy\",\n",
    "            env = env,\n",
    "            gae_lambda = 0.9,\n",
    "            gamma = 0.99,\n",
    "            learning_rate = 0.00096,\n",
    "            max_grad_norm = 0.5,\n",
    "            n_steps = 8,\n",
    "            vf_coef = 0.4,\n",
    "            ent_coef = 0.0,\n",
    "            tensorboard_log = \"./tensorboard\",\n",
    "            policy_kwargs=dict(\n",
    "            log_std_init=-2, ortho_init=False),\n",
    "            normalize_advantage=False,\n",
    "            use_rms_prop= True,\n",
    "            use_sde= True,\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84772d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes tens of minutes\n",
    "model.learn(2_000_000)\n",
    "# Save the model and  VecNormalize statistics when saving the agent\n",
    "model.save(\"a2c-AntBulletEnv-v0\")\n",
    "env.save(\"vec_normalize.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808f71e6",
   "metadata": {},
   "source": [
    "End Huggingface stuff.\n",
    "\n",
    "Now let's do it with cartpole to see how quickly it grasps a simple env.\n",
    "\n",
    "Was going to adapt the huggingface stuff but this is from the stable baselines website verbatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bc4d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "#from stable_baselines3.common.policies import MlpPolicy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "# Parallel environments\n",
    "env = make_vec_env('CartPole-v1', n_envs=4)\n",
    "\n",
    "model = A2C(\"MlpPolicy\", env, verbose=1)\n",
    "for i in range(1000):\n",
    "    model.learn(total_timesteps=1000)\n",
    "\n",
    "    mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "    if mean_reward > 110:\n",
    "        print(f\"took {i * 1000} timesteps to reach mean reward 100\")\n",
    "        break;\n",
    "#model.learn(total_timesteps=100000)\n",
    "model.save(\"a2c_cartpole\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = A2C.load(\"a2c_cartpole\")\n",
    "\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d2d905",
   "metadata": {},
   "source": [
    "OK now let's try Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169e244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "#from stable_baselines3.common.policies import MlpPolicy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "# Parallel environments\n",
    "env = make_vec_env('ALE/Pong-v5', n_envs=4)\n",
    "\n",
    "model = A2C(\"CnnPolicy\", env, verbose=1)\n",
    "#model = A2C.load(\"a2c_pong\")\n",
    "\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%Y%m%d%H%M\")\n",
    "save_file = f\"a2c_pong-{dt_string}\"\n",
    "print(f\"Save file: {save_file}\")\n",
    "\n",
    "reward_milestones = []\n",
    "reward_target = -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e343e63e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 100_000\n",
    "for i in range(1000):\n",
    "    model.learn(total_timesteps=n, reset_num_timesteps=False)\n",
    "\n",
    "    mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "    print(f\"Now at {n * (i+1)} timesteps\")\n",
    "    if mean_reward > reward_target:\n",
    "        print(f\"took {n * (i+1)} timesteps to reach mean reward {reward_target}\")\n",
    "        reward_milestones.append({\"reward\": reward_target, \"timesteps\": (n*(i+1))})\n",
    "        reward_target+=1\n",
    "        break;\n",
    "\n",
    "#model.learn(total_timesteps=1_000_000)\n",
    "\n",
    "model.save(save_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1459eaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reward_milestones)\n",
    "print(f\"Next reward target: {reward_target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52862660",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del model # remove to demonstrate saving and loading\n",
    "\n",
    "#model = A2C.load(\"a2c_pong\")\n",
    "\n",
    "obs = env.reset()\n",
    "for i in range(100000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93869e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
